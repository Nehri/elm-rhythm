Accomplishments:

1) Ports
	We set up two ports in our code, both of which listen for information from Javascript. 
	Initially, we began this project by using Native for interacting with Javascript, but
	with the lack of documentation and the inability to keep a global state for callback
	functions that our library required, we decided to work with ports instead.
	The first port we made (named ampharos) sends real time amplitude and frequency data
	from js to elm. This data is used for our background visualization of the music.
	The second port only sends data at the beginning of each playback. We treat
	this data as if it were our initial state, as it contains info, such as bpm, that the
	program needs to start the game, but that only needs to be sent once.

2) Sound processing
	As Elm does not currently have libraries for sound processing (or even playing,
	Elm.Audio is deprecated), we had to use a js library named p5 to send sound data to Elm.
	We did not write this library, but we made bug fixes to it along the way as it is
	not a well-maintained library. Had we known this at the beginning, we may have chosen
	a different library to build our code off of.

	We initially wanted to do all of our sound processing in real time, but due to lag and
	the need to print dots before they actually occur in the music so that players can hit
	them with the beat, it became necessary to pre-process the peaks data and then send it all 
	to elm once the processing had finished. This is partially due to how we deal with p5's 
	inaccurate peak detection at the beginning of the sound file by doing both normal peak 
	detection and peak detection of a reversed version of the soundfile. While this is fairly 
	expensive, it happens just once and greatly increased the accuracy of our peak data.
	We still do amplitude and frequency analysis in real time, as they did not cause significant
	lag.

3) Timestamp management
	Since we switched to pre-processed data, and had to deal with significant lag, we needed
	to use timestamps on our data so that we could have events occur correctly relative to
	each other, and be able to recover if elm got out of sync with the music.
	The time stamps sent from p5 were in seconds relative to the start of song, but all other
	time stamps were in milliseconds of system time, so some conversion was necessary
	to handle timed events. Moreover, in order to prevent accumulating lag, we needed to use
	the fps signal (which has time deltas instead of system time) to update according to the
	time passed, which is frequently not in the intervals promised by Elm.

	Perhaps the most important use of timestamps occurs when we begin to output peaks (beats)
	on the screen before they must be hit by the player. To do this, we must have the data 
	beforehand, and so we use asynchronous processing to get the whole list of peaks at the
	beginning of song and iterate through it based on both time and the timestamps of 
	click inputs. To output the peaks ahead of time, we compare the start time of the song
	to the timedelta of the peak and draw the dot at the position the line will be in when
	the timedelta occurs.

4) Player input
	The only player input we use is from the spacebar, but since we need to account for
	reaction time and lag, we have a buffered time zone where space bars are counted as
	correctly aligned with a given peak. Making this zone feel correct took quite a bit
	of experimentation, and unfortunately is likely system specific. If we were to continue
	this project, we would want to include a calibration test.

5) Visual display
	Our display is made up of three parts:
	The background, which is made of 5 circles, is a visualization of the song's realtime
	data. From left to right, each circle represts a frequency band, with its radius
	representing the energy (amplitude over a frequency range) in that band. Thus the
	left cirlces pulse with the beat of the song, and the others grow and fade depending
	on the volume and instrumentation currently being used.
	The bar, which moves up and down with the tempo of the song. Assuming that p5 correctly
	calculates the song's bpm (which it normally at least gets close with) the bar moves from
	one end to another every 4 beats.


Our experience with Elm:


Other challenges:
	Aaron's inability to spell caused one minor setback when loading the Javascript file on 
	the page silently failed because the file was named differently from what the html was
	searching for.

Known bugs:


Progress:
	We spent most of our time trying to get elm to interact with javascript so that we could use existing libraries (we are using p5.sound) to interact with and analyze sound files, as elm is currently lacking those abilities. We first tried to do this by writing Native elm in javascript, but due to difficulties with asynchronous functions in the music parsing javascript library we are using, we had to switch to using Ports.

	Currently we have set up one port in elm that listens for incoming sound data from javascript. We then run javascript (found in out.html) that plays the sound file and sends amplitude and energy data to elm every 66 milliseconds so that we have enough real time data for 20fps.

	For now, our visualization of the sound is 5 circles, each of which corresponds to a frequency range, with lower frequencies on the left and higher frequencies on the right. Each circle's radius is derived from the energy in its corresponding frequency range, so circles will pulse with the music. This is mostly a proof of concept that we can get sound data to elm and then display something with it, but the ones on the left are still quite fun to watch as they pulse with the beat of the song.


Future Plans:
	Now that we have gotten the port infrastructure created, we should be able to make progress much more quickly. We would like to proceed with our goal of making a rhythm game out of the sound data we are sending to elm. To do so we will need to add a player interface, do more signal processing, and deal with timing and lag issues.

	For the player interface, we will mark key sound events (likely peaks in amplitude or energy) and have the player press keyboard keys in rhythm with those events. To do so we will create a infrastructure that keeps track of  which times keys are pressed, and then assigns points if the player pressed the key within a certain range of the event.

	Most of the signal processing needed can be done by making further use of javascript libraries, but if we are feeling ambitious and lag is not an issue, we could try to do some in elm. This would be a stretch goal, and if memory becomes an issue may be impossible.

	While elm can mostly keep up with the real time sound data we are sending it, we have noticed some lag. To deal with this we will put a time gap between analysis and display, and send timestamps with the data so that if there is lag we can still attempt to synchronize elm's events with the sound. As lag becomes a larger problem as the complexity of our program increases, we will also filter data at the javascript level so that elm does not have to process data that would be discarded anyways.